{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base libraries for mathematical operations, dataframes, time and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "font = {'family' : 'sans-serif',\n",
    "        'style' : 'normal',\n",
    "        'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_plots\n",
    "from py_plots import precisionmeasures as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 1px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Starting time\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics_table(test,pred,feature):\n",
    "    '''Inputs:\n",
    "            test = actual labels of test set\n",
    "            pred = model predictions for the the test set\n",
    "            feature = feature name\n",
    "            \n",
    "            Computes macro- and micro- precision, recall and F1-score\n",
    "        Output:\n",
    "            Multi-index data frame with 3 precision measures \n",
    "    '''\n",
    "    temp_dict = {'Performance':['Precision','Recall','F1-Score']}\n",
    "    averages = ['micro','macro']\n",
    "    for average in averages:\n",
    "        p,r,f,_ = prfs(test,pred,average = average)\n",
    "        temp_dict[average]= np.round((p,r,f),4)\n",
    "    temp_df = pd.DataFrame(temp_dict)\n",
    "    temp_df = pd.melt(temp_df, id_vars=['Performance'], value_vars=averages,\n",
    "                        var_name='Metric', value_name=feature).set_index(['Metric','Performance'])\n",
    "    temp_df = temp_df.rename_axis([None,'Performance Measures'])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Hate','Offensive','Neutral']\n",
    "path = \"datasets/balanced_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#sikh #temple vandalised in in #calgary, #wso ...</td>\n",
       "      <td>sikh temple vandalised in in calgary wso conde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@user @user @user on flipside of  , praise @us...</td>\n",
       "      <td>on flipside of praise for reminder that reales...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @KatiePavlich: Charlie Crist doesn't have a...</td>\n",
       "      <td>charlie crist doesn t have any more political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@user you might be a libtard if... #libtard  #...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @RihannaHasAids: aight game over. dykes had...</td>\n",
       "      <td>aight game over dykes had to ruin it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                              tweet  \\\n",
       "0       0  #sikh #temple vandalised in in #calgary, #wso ...   \n",
       "1       2  @user @user @user on flipside of  , praise @us...   \n",
       "2       2  RT @KatiePavlich: Charlie Crist doesn't have a...   \n",
       "3       0  @user you might be a libtard if... #libtard  #...   \n",
       "4       0  RT @RihannaHasAids: aight game over. dykes had...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  sikh temple vandalised in in calgary wso conde...  \n",
       "1  on flipside of praise for reminder that reales...  \n",
       "2  charlie crist doesn t have any more political ...  \n",
       "3  you might be a libtard if libtard sjw liberal ...  \n",
       "4               aight game over dykes had to ruin it  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the dataset\n",
    "data = pd.read_csv(path)\n",
    "# drop any rows with null (after preprocessing)\n",
    "data = data.dropna()\n",
    "# print first 5 rows of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split dataset into training-validation-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum lenght (word-count) of tweets in the training set: 34\n",
      "\n",
      "==============================\n",
      "Training-Validation-Test Split\n",
      "==============================\n",
      "Size of training data: 8865\n",
      "..............................\n",
      "Size of validation data: 4367\n",
      "..............................\n",
      "Size of test data: 6518\n",
      "..............................\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets (2:1)\n",
    "X_train, x_test, Y_train, y_test = train_test_split(data.clean_tweet, data.labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# maximum word count of tweets in the training set\n",
    "max_length = np.max([len(tweet.split()) for tweet in X_train])\n",
    "\n",
    "print('Maximum lenght (word-count) of tweets in the training set: {}\\n'.format(max_length))\n",
    "\n",
    "\n",
    "# Split the trainng dataset further into training and validation sets (2:1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_val_onehot = to_categorical(y_val)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "# Print\n",
    "print('=='*15)\n",
    "print('Training-Validation-Test Split')\n",
    "print('=='*15)\n",
    "print('Size of training data: {}'.format(len(y_train)))\n",
    "print('..'*15)\n",
    "print('Size of validation data: {}'.format(len(y_val)))\n",
    "print('..'*15)\n",
    "print('Size of test data: {}'.format(len(y_test)))\n",
    "print('..'*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenizaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of training (including validation set) vocabulary: 18602 words\n"
     ]
    }
   ],
   "source": [
    "# Initializer tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train.append(x_val))\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Total size of training (including validation set) vocabulary: {} words'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Zero-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_train = tokenizer.texts_to_sequences(x_train)\n",
    "padded_train = pad_sequences(sequence_train, maxlen=max_length, padding='post') \n",
    "\n",
    "sequence_val = tokenizer.texts_to_sequences(x_val)\n",
    "padded_val = pad_sequences(sequence_val, maxlen=max_length, padding='post') \n",
    "\n",
    "sequence_test = tokenizer.texts_to_sequences(x_test)\n",
    "padded_test = pad_sequences(sequence_test, maxlen=max_length, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix is 18602 x 300\n"
     ]
    }
   ],
   "source": [
    "# Upload embedding matrix for words in the vocabulary\n",
    "embed_dim = 300\n",
    "embedding_matrix = pd.read_pickle(\"model/GloVe_matrix.pkl\").values\n",
    "print( 'Shape of embedding matrix is {} x {}'. format(embedding_matrix.shape[0],embedding_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overfit-Hyperparamters Tuning of Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finalizing the # of layers, # of neurons, batch size and activation function\n",
    "\n",
    "1. Dropout rate\n",
    "        -0, 0.25, 0.50\n",
    "\n",
    "2. Kernel Regularizar (L2 norm, only)\n",
    "        -0, 0.1, 0.01, 0.001\n",
    "\n",
    "3. Weight Constraint (max norm, only)\n",
    "           -1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel_reg = 0, dropout_rate = 0., weight_constraint = 0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(kernel_reg),\n",
    "                           kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(kernel_reg),\n",
    "                           kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(kernel_reg),\n",
    "                           kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(8, activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(kernel_reg),\n",
    "                           kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 30, verbose = 0)\n",
    "\n",
    "dropout_rate= [0,0.25,0.50]\n",
    "kernel_reg = [0,0.1,0.01,0.001]\n",
    "weight_constraint = [1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = dict(kernel_reg= kernel_reg,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  weight_constraint=weight_constraint)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, cv =5, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',patience=2)\n",
    "\n",
    "grid_result = grid_result.fit(padded_train,y_train_onehot,\n",
    "                           epochs=30,\n",
    "                           batch_size=128,\n",
    "                           validation_data=(padded_val, y_val_onehot),\n",
    "                           callbacks=[es],\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# Model predictions\n",
    "pred = grid_result.predict(padded_test) \n",
    "tbl = performance_metrics_table(y_test,y_pred,'Final best MLP overfit parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=='*10, 'Best MLP Model fits', '=='*10)\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
