{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base libraries for mathematical operations, dataframes, time and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "font = {'family' : 'sans-serif',\n",
    "        'style' : 'normal',\n",
    "        'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_plots\n",
    "from py_plots import precisionmeasures as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics_table(test,pred,feature):\n",
    "    '''Inputs:\n",
    "            test = actual labels of test set\n",
    "            pred = model predictions for the the test set\n",
    "            feature = feature name\n",
    "            \n",
    "            Computes macro- and micro- precision, recall and F1-score\n",
    "        Output:\n",
    "            Multi-index data frame with 3 precision measures \n",
    "    '''\n",
    "    temp_dict = {'Performance':['Precision','Recall','F1-Score']}\n",
    "    averages = ['micro','macro']\n",
    "    for average in averages:\n",
    "        p,r,f,_ = prfs(test,pred,average = average)\n",
    "        temp_dict[average]= np.round((p,r,f),4)\n",
    "    temp_df = pd.DataFrame(temp_dict)\n",
    "    temp_df = pd.melt(temp_df, id_vars=['Performance'], value_vars=averages,\n",
    "                        var_name='Metric', value_name=feature).set_index(['Metric','Performance'])\n",
    "    temp_df = temp_df.rename_axis([None,'Performance Measures'])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Hate','Offensive','Neutral']\n",
    "path = \"datasets/balanced_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the dataset\n",
    "data = pd.read_csv(path)\n",
    "# drop any rows with null (after preprocessing)\n",
    "data = data.dropna()\n",
    "# print first 5 rows of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split dataset into training-validation-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets (2:1)\n",
    "X_train, x_test, Y_train, y_test = train_test_split(data.clean_tweet, data.labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# maximum word count of tweets in the training set\n",
    "max_length = np.max([len(tweet.split()) for tweet in X_train])\n",
    "\n",
    "print('Maximum lenght (word-count) of tweets in the training set: {}\\n'.format(max_length))\n",
    "\n",
    "\n",
    "# Split the trainng dataset further into training and validation sets (2:1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_val_onehot = to_categorical(y_val)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "# Print\n",
    "print('=='*15)\n",
    "print('Training-Validation-Test Split')\n",
    "print('=='*15)\n",
    "print('Size of training data: {}'.format(len(y_train)))\n",
    "print('..'*15)\n",
    "print('Size of validation data: {}'.format(len(y_val)))\n",
    "print('..'*15)\n",
    "print('Size of test data: {}'.format(len(y_test)))\n",
    "print('..'*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenizaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train.append(x_val))\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Total size of training (including validation set) vocabulary: {} words'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Zero-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_train = tokenizer.texts_to_sequences(x_train)\n",
    "padded_train = pad_sequences(sequence_train, maxlen=max_length, padding='post') \n",
    "\n",
    "sequence_val = tokenizer.texts_to_sequences(x_val)\n",
    "padded_val = pad_sequences(sequence_val, maxlen=max_length, padding='post') \n",
    "\n",
    "sequence_test = tokenizer.texts_to_sequences(x_test)\n",
    "padded_test = pad_sequences(sequence_test, maxlen=max_length, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload embedding matrix for words in the vocabulary\n",
    "embed_dim = 300\n",
    "embedding_matrix = pd.read_pickle(\"model/GloVe_matrix.pkl\").values\n",
    "print( 'Shape of embedding matrix is {} x {}'. format(embedding_matrix.shape[0],embedding_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  6-Hidden layers\n",
    "\n",
    "Parameter Grid:\n",
    "\n",
    "    - Hidden Layer 1: # of neurons [512, 256]\n",
    "    - Hidden Layer 2: # of neurons [256, 128]\n",
    "    - Hidden Layer 3: # of neurons [128, 64]\n",
    "    - Hidden Layer 4: # of neurons [64, 16]\n",
    "    - Hidden Layer 5: # of neurons [16, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neuronsHL1 = 256, neuronsHL2 = 128, neuronsHL3 = 64, neuronsHL4 = 16, neuronsHL5 = 8):\n",
    "    print('=='*15)\n",
    "    print(neuronsHL1, neuronsHL2, neuronsHL3, neuronsHL4, neuronsHL5)\n",
    "    print('=='*15)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronsHL1, activation='relu'))\n",
    "    model.add(Dense(neuronsHL2, activation='relu'))\n",
    "    model.add(Dense(neuronsHL3, activation='relu'))\n",
    "    model.add(Dense(neuronsHL4, activation='relu'))\n",
    "    model.add(Dense(neuronsHL5, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 30, verbose = 0)\n",
    "\n",
    "param_grid = dict(neuronsHL1=[512,256],\n",
    "                  neuronsHL2=[256,128],\n",
    "                  neuronsHL3=[128,64],\n",
    "                  neuronsHL4=[64,16],\n",
    "                  neuronsHL5=[16,8])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, cv =5, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',patience=1)\n",
    "\n",
    "grid_result = grid.fit(padded_train,y_train_onehot,epochs=30,batch_size=128,\n",
    "                        validation_data=(padded_val, y_val_onehot),\n",
    "                       callbacks=[es], verbose=2)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Model predictions\n",
    "pred = grid_result.predict(padded_test)\n",
    "tbl = performance_metrics_table(y_test,pred,'Best 6 layer fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  4-Hidden layers\n",
    "\n",
    "Parameter Grid:\n",
    "\n",
    "    - Hidden Layer 1: # of neurons [128, 64]\n",
    "    - Hidden Layer 2: # of neurons [64,32]\n",
    "    - Hidden Layer 3: # of neurons 16\n",
    "    - Hidden Layer 4: # of neurons 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neuronsHL1 = 128, neuronsHL2 = 64):\n",
    "    print('=='*15)\n",
    "    print(neuronsHL1, neuronsHL2)\n",
    "    print('=='*15)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronsHL1, activation='relu'))\n",
    "    model.add(Dense(neuronsHL2, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 30, verbose = 0)\n",
    "\n",
    "param_grid = dict(neuronsHL1=[128,64],\n",
    "                  neuronsHL2=[64,32])\n",
    "\n",
    "grid = GridSearchCV(estimator=model, cv =5, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',patience=1)\n",
    "\n",
    "grid_result = grid.fit(padded_train,y_train_onehot,epochs=30,batch_size=128,\n",
    "                        validation_data=(padded_val, y_val_onehot),\n",
    "                       callbacks=[es], verbose=2)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Model predictions\n",
    "pred = grid_result.predict(padded_test)\n",
    "tbl = tbl.join(performance_metrics_table(y_test,pred,'Best 4 layer fit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  3-Hidden layers\n",
    "\n",
    "Parameter Grid:\n",
    "\n",
    "    - Hidden Layer 1: # of neurons [64, 32]\n",
    "    - Hidden Layer 2: # of neurons [16, 8]\n",
    "    - Hidden Layer 3: # of neurons [128, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neuronsHL1 = 64, neuronsHL2 = 8):\n",
    "    print('=='*15)\n",
    "    print(neuronsHL1, neuronsHL2)\n",
    "    print('=='*15)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(neuronsHL1, activation='relu'))\n",
    "    model.add(Dense(neuronsHL2,, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 30, verbose = 0)\n",
    "\n",
    "param_grid = dict(neuronsHL1=[64,32],\n",
    "                  neuronsHL2=[16,8])\n",
    "\n",
    "grid = GridSearchCV(estimator=model, cv =5, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',patience=1)\n",
    "\n",
    "grid_result = grid.fit(padded_train,y_train_onehot,epochs=30,batch_size=128,\n",
    "                        validation_data=(padded_val, y_val_onehot),\n",
    "                       callbacks=[es], verbose=2)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# Model predictions\n",
    "pred = grid_result.predict(padded_test)\n",
    "tbl = tbl.join(performance_metrics_table(y_test,pred,'Best 3 layer fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=='*10, 'Best MLP Model fits', '=='*10)\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
